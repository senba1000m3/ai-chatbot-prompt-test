# 模型準確率評分機制 v4.0

統一使用 GPT-4o 模型進行評分，並結合歷史表現與使用者回饋，創造一個更動態且公平的評分系統。
為提供更客觀、全面且與時俱進的語言模型效能評估，我們實作了一套混合評分系統，它結合了 **AI 多維度評估**、**演化式歷史表現**與**動態使用者回饋加權**。最終的「準確率」分數由一個透明的公式計算得出，使整個過程清晰且可控。

## 第一步：AI 多維度評估

當觸發準確率計算時，系統會請求一個獨立的評估模型（`gpt-4o`），在考量模型前次準確率的同時，從四個核心維度對最新的回覆進行評分（0-100分）。

1.  **指令遵循度 (Instruction Adherence):**
    *   模型是否嚴格遵守了系統提示 (System Prompt) 中的所有指令（例如：回覆格式、角色扮演、內容限制等）？

2.  **事實準確性 (Factual Accuracy):**
    *   回覆的內容是否真實、準確，沒有捏造或誤導性的資訊？

3.  **問題相關性 (Relevance):**
    *   回覆是否精準地命中了使用者問題的核心，沒有答非所問？

4.  **綜合品質 (Overall Quality):**
    *   綜合考量語言的流暢度、語氣、結構、安全性等因素後的整體評價。此分數是計算最終準確率的關鍵基礎。

評估模型還會提供一句話的**評分理由 (Reasoning)**，以說明其給出「綜合品質」分數的主要原因。

## 第二步：演化式基礎分計算

為確保準確率分數能反映模型長期的表現，而非單次對話的結果，我們計算了一個演化式的基礎分數。此分數是模型**前次準確率**與本次評估的**綜合品質分數**的加權平均值。

> **基礎分數 = (前次準確率 × 0.3) + (本次綜合品質分數 × 0.7)**

此公式創造了一種「平滑」效果，使單次的優異或不佳表現不會劇烈影響總分，但長期的表現趨勢仍會被反映出來。

## 第三步：動態使用者回饋加權 (評價調整)

使用者的評價（`good` / `bad`）是最直接的表現指標，其影響力會根據模型當前的「基礎分數」動態調整，而非固定增減。

*   **正面激勵 (`good`)**: 獎勵分數與模型的「進步空間」成正比。分數越低的模型，單次好的表現能獲得越高的獎勵。
    > **獎勵 = (100 - 基礎分數) × 0.15**

*   **負面懲罰 (`bad`)**: 懲罰分數與模型的「當前分數」成正比。分數越高的模型，單次差的表現所受到的懲罰越重。
    > **懲罰 = 基礎分數 × 0.30**

## 第四步：最終準確率計算

最終的準確率分數由以下公式決定：

> **最終準確率 = 基礎分數 + 總獎勵分數 - 總懲罰分數**

計算結果將被限制在 **0 到 100** 的區間內。

---

**總結**：此 v4.0 機制創造了一個更嚴謹、更動態的分數。它不僅利用 AI 進行客觀分析、結合歷史表現，更讓使用者回饋的權重能根據模型的表現水準動態變化，使得評分更具挑戰性與公平性。
